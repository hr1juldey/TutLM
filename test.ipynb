{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import time\n",
    "\n",
    "from tools import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from rake_nltk import Rake\n",
    "\n",
    "class GraphRAG:\n",
    "    def __init__(self, graph, k=25):\n",
    "        self.graph = graph\n",
    "        self.k = k\n",
    "\n",
    "    def find_most_relevant_chunks(self, query: str):\n",
    "        \"\"\"Find the most relevant chunks based on the graph and cosine similarity to the query.\"\"\"\n",
    "        # Step 1: Extract keywords from the query using RAKE\n",
    "        r = Rake()\n",
    "        r.extract_keywords_from_text(query)\n",
    "        keywords = r.get_ranked_phrases()\n",
    "\n",
    "        # Step 2: Find relevant sentences in the graph based on keywords\n",
    "        relevant_sentences = set()\n",
    "        for keyword in keywords:\n",
    "            for node in self.graph.nodes():\n",
    "                if keyword.lower() in node.lower():  # Check if keyword is in the node\n",
    "                    relevant_sentences.add(node)  # Add the whole sentence\n",
    "\n",
    "        # Step 3: Calculate embeddings for relevant sentences\n",
    "        similarities = {}\n",
    "        query_embedding = get_embedding(query)\n",
    "\n",
    "        for sentence in relevant_sentences:\n",
    "            if sentence in self.graph.nodes:\n",
    "                embedding = self.graph.nodes[sentence].get('embedding')\n",
    "                if embedding is not None:\n",
    "                    cosine_sim = calculate_cosine_similarity(sentence, query_embedding, embedding)\n",
    "                    similarities[sentence] = cosine_sim[1]  # Store only the similarity score\n",
    "\n",
    "        # Sort sentences by similarity\n",
    "        sorted_sentences = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "        return sorted_sentences[:self.k]  # Return top k relevant sentences\n",
    "    \n",
    "    def answer_query(self, query: str, Ucontext: Optional[List[str]] = None):\n",
    "        \"\"\"Answer a query using the graph and embeddings, combining both system context and user-provided context.\"\"\"\n",
    "        \n",
    "        # Find the most relevant chunks from the graph\n",
    "        relevant_chunks = self.find_most_relevant_chunks(query)\n",
    "        \n",
    "        # Join the relevant chunks from the graph into a single string\n",
    "        graph_context = \" \".join(chunk for chunk, _ in relevant_chunks)\n",
    "        \n",
    "        # If Ucontext is provided, join it with the graph context\n",
    "        if Ucontext:\n",
    "            user_context = \" \".join(Ucontext)\n",
    "            context = f\"{user_context} {graph_context}\"  # Combine user context and graph context\n",
    "        else:\n",
    "            context = graph_context\n",
    "        \n",
    "        # Generate the response using the combined context\n",
    "        response = ollama.generate(model='mathstral', prompt=f\"Based on the Context: {context} answer the Question: {query}\")\n",
    "        \n",
    "        # Return the response if available, otherwise return a default message\n",
    "        return response.get('response', \"No answer generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming 'my_graph' is your pre-defined graph structure.\n",
    "path=\"/home/riju279/Documents/Code/TutorLM/tmp/bioprocess-engineering-principles-doran3.gml\"\n",
    "my_graph=load_graph(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_rag = GraphRAG(graph=my_graph)\n",
    "\n",
    "# Example question and optional user context.\n",
    "my_question = \"Calculate the dilution rate required to maintain a steady state in a continuous stirred-tank reactor (CSTR) with a volume of 1000 L and a desired biomass concentration of 2 g/L if the feed concentration is 10 g/L.\"\n",
    "user_context = [\n",
    "\n",
    "\"Volume of the CSTR (in liters)\",  # 1000 L\n",
    "    \"Desired biomass concentration (in g/L)\",  # 2 g/L\n",
    "    \"Feed concentration (in g/L)\",  # 10 g/L\n",
    "    \"Biomass yield coefficient (Y_x/s)\",  # This might be needed depending on biomass production rate\n",
    "    \"Specific growth rate (μ)\",  # Specific growth rate of the biomass in the reactor\n",
    "    \"Rate of biomass formation (dx/dt)\",  # Rate of biomass production or depletion\n",
    "    \"Steady-state condition assumption\",  # Assumes the system is at steady state where dx/dt = 0\n",
    "    \"Dilution rate (D)\",  # The variable we need to calculate\n",
    "    \"Substrate concentration (S)\",  # Initial and final substrate concentrations in the reactor\n",
    "    \"Residence time\",  # 1/D, related to how long the biomass remains in the reactor\n",
    "    \"Monod equation\",  # Used to relate specific growth rate to substrate concentration\n",
    "    \"Mass balance equation for CSTR\",  # Basic balance: Input - Output = Accumulation\n",
    "    \"Reactor assumptions\",  # Assuming ideal mixing, no biomass loss\n",
    "\n",
    "]\n",
    "\n",
    "# Get the answer.\n",
    "answer = graph_rag.answer_query(query=my_question, Ucontext=user_context)\n",
    "\n",
    "# Print the answer.\n",
    "print(f\"\\n\\n Question: {my_question}\")\n",
    "print(f\"\\n\\n   ---------------------------------------------------------  \\n\\nAnswer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experimental code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from typing import List, Optional\n",
    "from rake_nltk import Rake\n",
    "\n",
    "class SubQA(dspy.Signature):\n",
    "    \"\"\"Break down the main question into smaller questions.\"\"\"\n",
    "    Iq = dspy.InputField(desc=\"The main question to break down.\")\n",
    "    Oq = dspy.OutputField(desc=\"A well formatted python list of related questions.\")\n",
    "\n",
    "class FindRelevantChunks(dspy.Signature):\n",
    "    \"\"\"Find relevant chunks based on a query and graph.\"\"\"\n",
    "    query = dspy.InputField(desc=\"The question to find relevant chunks for.\")\n",
    "    graph = dspy.InputField(desc=\"The graph containing nodes with embeddings.\")\n",
    "    k = dspy.InputField(default=25)\n",
    "    relevant_chunks = dspy.OutputField(desc=\"Top k relevant chunks from the graph.\")\n",
    "\n",
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Generate an answer based on context with detailed explanation.\"\"\"\n",
    "    context = dspy.InputField(desc=\"Combined context from user and graph.\")\n",
    "    question = dspy.InputField(desc=\"The original question.\")\n",
    "    answer = dspy.OutputField(desc=\"The generated answer with detailed explanation.\")\n",
    "\n",
    "\n",
    "class ImgQA(dspy.Signature):\n",
    "    \"\"\"Describe the image in the best possible way with detail and accuracy\"\"\"\n",
    "    image=dspy.InputField(desc=\"can be an image or a path to an image\")\n",
    "    description=dspy.OutputField(desc=\"detailed description of the image\")\n",
    "\n",
    "\n",
    "class Router(dspy.Signature):\n",
    "    \"\"\"You are given a block of markdown text. Your task is to classify each part of the text into one of the following categories General text,Math,Code,Image\"\"\"\n",
    "\n",
    "    text=dspy.InputField(desc=\"can be any type of markdown or non markdown text\")\n",
    "    general=dspy.OutputField(desc=\"anything that is not code,math equation or an image or path to an image\")\n",
    "    maths=dspy.OutputField(desc=\"Mathematical expressions, equations, or symbols that can be enclosed between $...$ or $$...$$ in markdown. or anything that has numbers for calculation\")\n",
    "    code=dspy.OutputField(desc=\"Programming code blocks or inline code. Code blocks are usually enclosed within triple backticks (```) and inline code with single backticks (`).\")\n",
    "    img=dspy.OutputField(desc=\"Image links are typically represented with the ![alt text](image-url) markdown syntax or end with ().png) (.jpg) (.jpeg) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=read_file(\"/home/riju279/Documents/Code/TutorLM/TutLM/QM.py\")\n",
    "\n",
    "prd=dspy.ChainOfThought(Router)\n",
    "\n",
    "op=prd(text=T)\n",
    "\n",
    "op.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "olm1 = dspy.LM(model=\"ollama/mistral-nemo:latest\", api_base=\"http://localhost:11434\")\n",
    "olm2=  dspy.LM(model=\"ollama/mathstral:latest\", api_base=\"http://localhost:11434\")\n",
    "olm3=  dspy.LM(model=\"ollama/llava:latest\", api_base=\"http://localhost:11434\")\n",
    "olm4= dspy.LM(model=\"ollama/deepseek-coder-v2:latest\", api_base=\"http://localhost:11434\")\n",
    "\n",
    "dspy.settings.configure(lm=olm1)\n",
    "gen=dspy.settings.context(lm=olm1)\n",
    "mat=dspy.settings.context(lm=olm2)\n",
    "vis=dspy.settings.context(lm=olm3)\n",
    "code=dspy.settings.context(lm=olm4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRAG(dspy.Module):\n",
    "    def __init__(self, graph,mode=\"gen\"):\n",
    "        super().__init__()\n",
    "\n",
    "        olm1 = dspy.LM(model=\"ollama/mistral-nemo:latest\", api_base=\"http://localhost:11434\")\n",
    "        olm2=  dspy.LM(model=\"ollama/mathstral:latest\", api_base=\"http://localhost:11434\")\n",
    "        olm3=  dspy.LM(model=\"ollama/llava:latest\", api_base=\"http://localhost:11434\")\n",
    "        olm4=  dspy.LM(model=\"ollama/deepseek-coder-v2:latest\", api_base=\"http://localhost:11434\")\n",
    "        # Step 3: Generate answer based on the selected mode\n",
    "        if mode == 'gen':\n",
    "             dspy.settings.context(lm=olm1)\n",
    "        elif mode == 'mat':\n",
    "             dspy.settings.context(lm=olm2)\n",
    "        elif mode == 'vis':\n",
    "             dspy.settings.context(lm=olm3)\n",
    "        elif mode == 'code':\n",
    "             dspy.settings.context(lm=olm4)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode: {mode}. Choose from 'gen', 'mat', 'vis', or 'code'.\")\n",
    "        \n",
    "        self.graph = graph\n",
    "        self.subqa_predictor = dspy.ChainOfThought(SubQA)\n",
    "        self.find_relevant_chunks_predictor = dspy.ChainOfThought(FindRelevantChunks)\n",
    "        self.generate_answer_predictor = dspy.ChainOfThought(GenerateAnswer)\n",
    "        self.mode=mode\n",
    "\n",
    "    def load_graph(self, filepath):\n",
    "        \"\"\"Load the graph from a specified file path using pickle.\"\"\"\n",
    "        graph = read_gml(filepath)\n",
    "        print(f\"Graph loaded from {filepath}\")\n",
    "        return graph\n",
    "\n",
    "    def get_embedding(self, text, model=\"mxbai-embed-large\"):\n",
    "        \"\"\"Get embedding for a given text using Ollama API.\"\"\"\n",
    "        response = ollama.embeddings(model=model, prompt=text)\n",
    "        return response[\"embedding\"]\n",
    "\n",
    "    def calculate_cosine_similarity(self, chunk, query_embedding, embedding):\n",
    "        \"\"\"Calculate cosine similarity between a chunk and the query.\"\"\"\n",
    "        if np.linalg.norm(query_embedding) == 0 or np.linalg.norm(embedding) == 0:\n",
    "            return (chunk, 0)  # Handle zero vectors\n",
    "        cosine_sim = np.dot(query_embedding, embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(embedding))\n",
    "        return (chunk, cosine_sim)\n",
    "\n",
    "    def find_most_relevant_chunks(self, query: str, graph=None, k: int = 25):\n",
    "        \"\"\"Find the most relevant chunks based on the query and graph.\"\"\"\n",
    "        r = Rake()\n",
    "        r.extract_keywords_from_text(query)\n",
    "        keywords = r.get_ranked_phrases()\n",
    "\n",
    "        relevant_sentences = set()\n",
    "        for keyword in keywords:\n",
    "            for node in self.graph.nodes():\n",
    "                if keyword.lower() in node.lower():\n",
    "                    relevant_sentences.add(node)\n",
    "\n",
    "        similarities = {}\n",
    "        query_embedding = self.get_embedding(query)\n",
    "\n",
    "        for sentence in relevant_sentences:\n",
    "            if sentence in self.graph.nodes:\n",
    "                embedding = self.graph.nodes[sentence].get('embedding')\n",
    "                if embedding is not None:\n",
    "                    cosine_sim = self.calculate_cosine_similarity(sentence, query_embedding, embedding)\n",
    "                    similarities[sentence] = cosine_sim[1]\n",
    "\n",
    "        sorted_sentences = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "        return sorted_sentences[:k]\n",
    "    \n",
    "    \n",
    "    #dspy.settings.configure(rm=find_most_relevant_chunks)\n",
    "    \n",
    "    def answer_query(self, query: str, user_context: Optional[List[str]] = None,mode:str=\"gen\"):\n",
    "        \"\"\"Answer a query using the graph and embeddings.\"\"\"\n",
    "        \n",
    "        olm1 = dspy.LM(model=\"ollama/mistral-nemo:latest\", api_base=\"http://localhost:11434\")\n",
    "        olm2=  dspy.LM(model=\"ollama/mathstral:latest\", api_base=\"http://localhost:11434\")\n",
    "        olm3=  dspy.LM(model=\"ollama/llava:latest\", api_base=\"http://localhost:11434\")\n",
    "        olm4=  dspy.LM(model=\"ollama/deepseek-coder-v2:latest\", api_base=\"http://localhost:11434\")\n",
    "\n",
    "        # Step 3: Generate answer based on the selected mode\n",
    "        if mode == 'gen':\n",
    "             dspy.settings.context(lm=olm1)\n",
    "        elif mode == 'mat':\n",
    "             dspy.settings.context(lm=olm2)\n",
    "        elif mode == 'vis':\n",
    "             dspy.settings.context(lm=olm3)\n",
    "        elif mode == 'code':\n",
    "             dspy.settings.context(lm=olm4)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode: {mode}. Choose from 'gen', 'mat', 'vis', or 'code'.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.mode=mode\n",
    "        # Find relevant chunks RAG\n",
    "        relevant_chunks_result = self.find_relevant_chunks_predictor(query=query, graph=self.graph,k=25)\n",
    "        \n",
    "        # Join relevant chunks\n",
    "        graph_context = find_most_relevant_chunks(query=relevant_chunks_result.relevant_chunks, graph=self.graph,k=25)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Combine user context if provided\n",
    "        context = f\"{' '.join(user_context)} {graph_context}\" if user_context else graph_context\n",
    "\n",
    "        time.sleep(.5)\n",
    "        \n",
    "        # Generate answer\n",
    "        self.mode=mode\n",
    "        answer_result = self.generate_answer_predictor(context=context, question=query)\n",
    "        \n",
    "        return answer_result.answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your graph\n",
    "loadpath=\"/home/riju279/Documents/Code/TutorLM/tmp/bioprocess-engineering-principles-doran3.gml\"\n",
    "G=load_graph(loadpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag = GraphRAG(graph=G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "Q=[\n",
    "    \"Calculate the yield of a fermentation process if 10 kg of glucose is consumed and 8 kg of ethanol is produced. What is the yield coefficient?\", \n",
    "\n",
    "    \"A bioreactor has a volume of 500 L. If the initial concentration of a substrate is 20 g/L and it decreases to 5 g/L after 24 hours, calculate the rate of substrate consumption.\",\n",
    "\n",
    "    \"If a cell culture has a specific growth rate of 0.1 h^-1, how long will it take for the cell concentration to double from an initial concentration of 1 x 10^6 cells/mL?\",\n",
    "\n",
    "    \"Given that the half-life of a drug in the body is 4 hours, calculate how much of a 100 mg dose remains in the body after 12 hours.\",\n",
    "\n",
    "    \"A reaction has an activation energy of 50 kJ/mol. Using the Arrhenius equation, calculate the rate constant at 37°C (310 K) if the rate constant at 25°C (298 K) is known to be 0.1 s^-1.\",\n",
    "\n",
    "    \"If a bioprocess operates at a maximum specific growth rate of 0.2 h^-1 and substrate concentration is limiting, what will be the maximum biomass concentration achievable in a continuous culture?\",\n",
    "\n",
    "    \"Calculate the dilution rate required to maintain a steady state in a continuous stirred-tank reactor (CSTR) with a volume of 1000 L and a desired biomass concentration of 2 g/L if the feed concentration is 10 g/L.\",\n",
    "\n",
    "    \"In an enzyme-catalyzed reaction, if the Km value is 5 mM and the substrate concentration is 15 mM, calculate the reaction velocity if Vmax is known to be 100 µmol/min.\",\n",
    "\n",
    "    \"A pharmaceutical compound has a distribution coefficient (log P) of 3.5. Estimate its permeability across biological membranes using relevant equations.\",\n",
    "\n",
    "    \"If a protein solution has an absorbance of 0.75 at 280 nm, calculate its concentration in mg/mL using the Beer-Lambert law, given that ε (extinction coefficient) is 1.0 mL/(mg·cm).\",\n",
    "\n",
    "    \"A batch bioreactor contains 500 L of culture with an initial cell concentration of 1 x 10^6 cells/mL. After 10 hours, the cell concentration reaches 1 x 10^9 cells/mL. Calculate the specific growth rate and the doubling time.\",\n",
    "    \n",
    "    \"Given a fermentation process with an initial glucose concentration of 100 g/L, the glucose concentration decreases to 10 g/L after 12 hours. If the product yield is 0.5 g of product per g of glucose, calculate the total product formed at the end of the fermentation.\",\n",
    "    \n",
    "    \"In a continuous stirred-tank reactor (CSTR), the volumetric flow rate is 0.5 L/h, and the reactor volume is 1000 L. If the substrate concentration in the feed is 50 g/L and the conversion is 90%, calculate the substrate concentration in the reactor.\",\n",
    "    \n",
    "    \"For a protein with a molecular weight of 50 kDa, calculate the number of moles and molecules in 10 mg of this protein. Assume Avogadro's number to be 6.022 x 10^23 molecules/mol.\",\n",
    "    \n",
    "    \"A bioreactor is operating at a dilution rate of 0.1 h^-1. If the maximum specific growth rate of the organism is 0.4 h^-1, what will be the steady-state biomass concentration in the reactor if the feed substrate concentration is 100 g/L?\",\n",
    "    \n",
    "    \"An enzyme with a turnover number (kcat) of 1000 s^-1 and a Km value of 1 mM is acting on a substrate with a concentration of 10 mM. Calculate the initial velocity of the reaction if the enzyme concentration is 0.1 µM.\",\n",
    "    \n",
    "    \"Calculate the oxygen transfer rate (OTR) in a 2000 L bioreactor if the oxygen concentration in the gas phase is 0.21 mol/L, the gas-liquid mass transfer coefficient (k_La) is 50 h^-1, and the dissolved oxygen concentration in the liquid is 2 mg/L.\",\n",
    "    \n",
    "    \"A biopharmaceutical drug has a first-order degradation rate constant of 0.03 day^-1 at 25°C. Calculate the shelf life of the drug (time until 90 percent of the drug remains).\",\n",
    "    \n",
    "    \"In a dialysis process, a protein with a molecular weight of 150 kDa is being separated from smaller molecules. If the diffusion coefficient of the protein is 1 x 10^-11 m^2/s, calculate the time required for the protein to diffuse across a 1 mm membrane.\",\n",
    "    \n",
    "    \"For a mammalian cell culture operating in a perfusion bioreactor, the perfusion rate is set to 1 reactor volume per day. If the biomass concentration inside the reactor is 10^7 cells/mL, calculate the cell retention efficiency if 5 x 10^6 cells/mL are found in the permeate.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "J=int(input(\"enter a number between 1 to 20\"))\n",
    "\n",
    "main_question_index = J - 1  # Assuming J is defined elsewhere\n",
    "main_question = Q[main_question_index]\n",
    "\n",
    "\n",
    "    \n",
    "sub_questions_result = graph_rag.subqa_predictor(Iq=main_question)\n",
    "\n",
    "print(sub_questions_result.Oq)\n",
    "\n",
    "# Convert Oq string into a proper Python list\n",
    "sub_questions =json.loads(sub_questions_result.Oq)                  # [question.strip('- ').strip() for question in sub_questions_result.Oq.split('-') if question]\n",
    "\n",
    "sub_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for sub_question in sub_questions:\n",
    "    thought_process_result = graph_rag.answer_query(query=sub_question,mode=\"gen\")\n",
    "    time.sleep(.5)\n",
    "    \n",
    "# Final answer generation\n",
    "final_answer_result = graph_rag.answer_query(query=main_question, user_context=[thought_process_result],mode=\"mat\")\n",
    "\n",
    "print(f\"---- \\n\\n The question is: \\n\\n {main_question} \\n\\n Final Answer is \\n \\n {final_answer_result} \\n\\n --------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olm1.inspect_history(n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
